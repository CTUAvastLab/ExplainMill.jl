var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"citation/#Citation","page":"Citation","title":"Citation","text":"","category":"section"},{"location":"citation/","page":"Citation","title":"Citation","text":"For citing, please use the following entry for the original paper:","category":"page"},{"location":"citation/","page":"Citation","title":"Citation","text":"@misc{mandlik2021milljl,\n      title={Mill.jl and JsonGrinder.jl: automated differentiable feature extraction for learning from raw JSON data}, \n      author={Simon Mandlik and Matej Racinsky and Viliam Lisy and Tomas Pevny},\n      year={2021},\n      eprint={2105.09107},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n}","category":"page"},{"location":"citation/","page":"Citation","title":"Citation","text":"and the following for the implementation (fill in the used version):","category":"page"},{"location":"citation/","page":"Citation","title":"Citation","text":"@software{mill2018,\n  author = {Tomas Pevny and Simon Mandlik},\n  title = {Mill.jl framework: a flexible library for (hierarchical) multi-instance learning},\n  url = {https://github.com/CTUAvastLab/Mill.jl},\n  version = {...},\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"<img class=\"display-light-only\" src=\"assets/logo.svg\" alt=\"ExplainMill.jl logo\" style=\"width: 40%;\"/>\n<img class=\"display-dark-only\" src=\"assets/logo-dark.svg\" alt=\"ExplainMill.jl logo\" /style=\"width: 40%;\">","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Run the following in REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add ExplainMill","category":"page"},{"location":"","page":"Home","title":"Home","text":"Julia v1.9 or later is required.","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"References: related literature\nCitation: preferred citation entries","category":"page"},{"location":"#Breaking-change","page":"Home","title":"Breaking change","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"As discussed below, I have removed the \"feature\" that dict with single child is ignored. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Below snippet fix cuckoo model and data trained on schema created by  JsonGrinder version 1.6.1 and reasonably below.","category":"page"},{"location":"","page":"Home","title":"Home","text":"function fixmodel(model)\n\tmm = @set model.ms.behavior.ms.enhanced.im = ProductModel((data = model[:behavior][:enhanced].im,), identity)\n\tmm = @set mm.ms.clipboard_changes.im = ProductModel((to = mm[:clipboard_changes].im,), identity)\n\tmm = @set mm.ms.signatures.im = ProductModel((name = mm[:signatures].im,), identity)\n\tmm = @set mm.ms.network.ms.domains.im = ProductModel((domain = mm[:network][:domains].im,), identity)\nend\n\nfunction fixdata(ds)\n\tdd = @set ds.data.behavior.data.enhanced.data = ProductNode((data = ds[:behavior][:enhanced].data,))\n\tdd = @set dd.data.clipboard_changes.data = ProductNode((to = dd[:clipboard_changes].data,))\n\tdd = @set dd.data.signatures.data = ProductNode((name = dd[:signatures].data,))\n\tdd = @set dd.data.network.data.domains.data = ProductNode((domain = dd[:network][:domains].data,))\nend","category":"page"},{"location":"#Discussion","page":"Home","title":"Discussion","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Should we explicitly model missing in categorical variable as an n + 2 item?\nRemove skipping of Dictionary in JsonGrinder and replace it with the IdentityModel()\nArrayModel in Mill should have a default for missing values and potentially ProductModel","category":"page"},{"location":"#ExplainMill.jl","page":"Home","title":"ExplainMill.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This library provides an explanation of hierarchical multi-instance learning models using the method of Shapley values. This explanation method works by randomly perturbing \"features\" of the sample and observing the output of the classifier. The rationale behind the method is that if the feature is non-informative than the output of the classifier should be non-sensitive to its perturbation (and vice-versa). To support hierarchical models, we define several types of \"daf\" explainers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"BagDaf explains Mill.BagNode by removing instances\nTreeDaf is just a container and does not explain anything\nArrayDaf explains dense matrices (Mill.ArrayNode{Matrix}) by removing","category":"page"},{"location":"","page":"Home","title":"Home","text":"features of all samples (instances). Explaining individual items in dense matrices can be easily added, but at the moment author (Pevnak) is more interested in the influence of a particular feature on the sample. Features are modified by setting them to zero. This method assumes that features are centred, which is considering the state of JsonGrinder unlikely. A better method would be to provide a set samples such that values of modified features are replaced with a random sample from the population.","category":"page"},{"location":"","page":"Home","title":"Home","text":"SparseArrayDaf explains sparse matrix by zeroing individual items of the","category":"page"},{"location":"","page":"Home","title":"Home","text":"random matrix.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let's demonstrate the usage. Assume that we want to explain sample ds with model model using 1000 random samples. After necessary setup the important line is ExplainMill.explain(ds, model, n = 1000) which does the explanation","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Mill, ExplainMill, SparseArrays, Flux\n\nan = ArrayNode(randn(2,5))\ncn = ArrayNode(sprand(2,5, 0.5))\nbn = BagNode(ProductNode((a = an, c = cn)), AlignedBags([1:2,3:5]))\ntn = ProductNode((a = an[1:2], b = bn))\nds = BagNode(tn, AlignedBags([1:2]))\nmodel = reflectinmodel(ds, d -> Dense(d,2),\n\t\td -> SegmentedMean(d),\n\t\tb = Dict(\"\" => d -> Dense(d,1)))\n\nexplained_ds = ExplainMill.explain(ds, model, n = 1000)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The output of the model has to have dimension one. You might find convenient to use the following code to select specific output","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Setfield\nnew_model = @set model.bm.m = Chain(model.bm, x -> x[1,:])","category":"page"},{"location":"#Peaking-under-the-hood","page":"Home","title":"Peaking under the hood","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The explanation starts by creating a structure holding a shapley values for individual nodes. The structure resembles the Mill structure, but instead of data it holds Duff.Daf statistics. For a ds, the structure is created as","category":"page"},{"location":"","page":"Home","title":"Home","text":"daf = ExplainMill.Daf(ds)","category":"page"},{"location":"","page":"Home","title":"Home","text":"then, in the loop we repeatedlu create a sub-sample, calculate the shapley value, and update the stats","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StatsBase, Duff\ndss, mask = sample(daf, ds)\nv = ExplainMill.scalar(model(dss))\nDuff.update!(daf, mask, v)","category":"page"},{"location":"","page":"Home","title":"Home","text":"once the sufficient statistic is calculated, we extract the mask which will allow to prune the sample (by default everything all true which means no pruning.) Masks from individual level is returned together with daf statistics as a second argument, which allows to easily link daf statistics to cooresponding items in mask and in hierarchical mask. Using CatViews package further simplifies the construction, as","category":"page"},{"location":"","page":"Home","title":"Home","text":"using CatViews\nmask, dafs = ExplainMill.masks_and_stats(daf)\ncatmask = CatView(tuple([d.m for d in dafs]...))\npvalue = CatView(tuple([Duff.UnequalVarianceTTest(d.d) for d in dafs]...))","category":"page"},{"location":"","page":"Home","title":"Home","text":"catmask now behaves like a single array and similarly the p-values of a test where means of samples when the item / feaure is present. This means and if if we change item in catmask, the change is propagated to mask and prune(ds, mask) return the sample without the corresponding item. Note that since statistics is effectively zero as it has not been updated, pvalues are NaNs","category":"page"},{"location":"","page":"Home","title":"Home","text":"Adding a support for new type of nodes (lists) is currently involved and it might undergo further changes. At the moment, it is simple to take it out of the explanation. For example if we want to remove PathNode from explanation, define constructor as Duff.Daf(::PathNode) = ExplainMill.SkipDaf()","category":"page"},{"location":"#Design-thoughts-on-logic-output","page":"Home","title":"Design thoughts on logic output","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We need to carry observations that will be exported downward, otherwise skipped explanation and export of arrays would not work properly. This can get into interesting situations, where \nsomething can be present because of the upper mask but missing because of the lower mask. In this case, I will emit missing","category":"page"}]
}
